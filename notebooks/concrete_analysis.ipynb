{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tnUjWKNFrtZ"
   },
   "source": [
    "# Concrete Image Classification with Transfer Learning\n",
    "# Binary classification using MobileNetV2 + Fine-tuning\n",
    "\n",
    "## 1. Introduction & Problem Statement\n",
    "\n",
    "### Objective\n",
    "Classify concrete surface images into two categories: Positive (defect/crack present) \n",
    "and Negative (no defect). This automated classification can assist in infrastructure \n",
    "inspection and maintenance workflows.\n",
    "\n",
    "### Dataset Overview\n",
    "- **Total Images:** 40,000 (20,000 per class)\n",
    "- **Source:** 458 high-resolution images (4032×3024) augmented to create dataset\n",
    "- **Image Size:** 227×227 pixels, RGB channels\n",
    "- **Split:** 70% train, 15% validation, 15% test\n",
    "- **Classes:** Negative (no defect), Positive (defect present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reproducibility Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reproducibility configured with seed=13\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed=13):\n",
    "    \"\"\"Ensure reproducible results across runs\"\"\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_seed(13)\n",
    "print(\"Reproducibility configured with seed=13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "DATA_ROOT = \"data\"\n",
    "SPLITS_DIR = \"splits\"\n",
    "OUTPUT_DIR = \"results\"\n",
    "\n",
    "os.makedirs(SPLITS_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def list_images(data_root):\n",
    "    \"\"\"List all images with their labels\"\"\"\n",
    "    data_root = pathlib.Path(data_root)\n",
    "    items = []\n",
    "    for label_name in sorted([\"Negative\", \"Positive\"]):\n",
    "        for ext in (\"*.jpg\", \"*.png\", \"*.jpeg\"):\n",
    "            for p in (data_root / label_name).glob(ext):\n",
    "                items.append((str(p), 0 if label_name==\"Negative\" else 1))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samples(data_root, n_samples=4):\n",
    "    \"\"\"Show sample images from each class\"\"\"\n",
    "    fig, axes = plt.subplots(2, n_samples, figsize=(12, 6))\n",
    "    \n",
    "    for class_idx, class_name in enumerate([\"Negative\", \"Positive\"]):\n",
    "        class_dir = pathlib.Path(data_root) / class_name\n",
    "        sample_images = list(class_dir.glob(\"*.jpg\"))[:n_samples]\n",
    "        \n",
    "        for i, img_path in enumerate(sample_images):\n",
    "            img = plt.imread(img_path)\n",
    "            axes[class_idx, i].imshow(img)\n",
    "            axes[class_idx, i].axis('off')\n",
    "            if i == 0:\n",
    "                axes[class_idx, i].set_title(f\"{class_name}\", fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/sample_images.png\", dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Display samples\n",
    "display_samples(DATA_ROOT, n_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display dataset statistics\n",
    "items = list_images(DATA_ROOT)\n",
    "labels = [lbl for _, lbl in items]\n",
    "\n",
    "print(f\"Total images: {len(items)}\")\n",
    "print(f\"Negative samples: {labels.count(0)}\")\n",
    "print(f\"Positive samples: {labels.count(1)}\")\n",
    "print(f\"Class balance: {labels.count(1) / len(labels):.2%} positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paths(items, seed=13, train=0.7, val=0.15, test=0.15):\n",
    "    \"\"\"Stratified split maintaining class balance\"\"\"\n",
    "    assert abs(train + val + test - 1.0) < 1e-6\n",
    "    rng = np.random.default_rng(seed)\n",
    "    items = np.array(items, dtype=object)\n",
    "    y = np.array([lbl for _, lbl in items])\n",
    "    idx = np.arange(len(items))\n",
    "    \n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for cls in np.unique(y):\n",
    "        cls_idx = idx[y==cls]\n",
    "        rng.shuffle(cls_idx)\n",
    "        n = len(cls_idx)\n",
    "        ntr = int(n * train)\n",
    "        nval = int(n * val)\n",
    "        train_idx.extend(cls_idx[:ntr])\n",
    "        val_idx.extend(cls_idx[ntr:ntr + nval])\n",
    "        test_idx.extend(cls_idx[ntr + nval:])\n",
    "    \n",
    "    return items[train_idx], items[val_idx], items[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits\n",
    "train_items, val_items, test_items = split_paths(items, seed=13)\n",
    "\n",
    "# Save splits to JSON for reproducibility\n",
    "for name, data in zip([\"train\", \"val\", \"test\"], [train_items, val_items, test_items]):\n",
    "    with open(f\"{SPLITS_DIR}/{name}.json\", \"w\") as f:\n",
    "        json.dump([[p, int(lbl)] for p, lbl in data], f, indent=2)\n",
    "\n",
    "print(f\"Train: {len(train_items)} images ({len(train_items)/len(items):.1%})\")\n",
    "print(f\"Val:   {len(val_items)} images ({len(val_items)/len(items):.1%})\")\n",
    "print(f\"Test:  {len(test_items)} images ({len(test_items)/len(items):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Pipeline & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(json_path, img_size=(224, 224), batch=64, shuffle=False, seed=13):\n",
    "    \"\"\"Create TensorFlow dataset with preprocessing\"\"\"\n",
    "    with open(json_path) as f:\n",
    "        pairs = json.load(f)\n",
    "    \n",
    "    paths = np.array([p for p, _ in pairs], dtype=object)\n",
    "    labels = np.array([int(l) for _, l in pairs], dtype=np.int32)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    \n",
    "    def load_and_preprocess(path, label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, img_size)\n",
    "        return img, label\n",
    "    \n",
    "    ds = ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(8192, seed=seed, reshuffle_each_iteration=True)\n",
    "    \n",
    "    return ds.batch(batch).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_ds = create_dataset(f\"{SPLITS_DIR}/train.json\", img_size=IMG_SIZE, \n",
    "                          batch=BATCH_SIZE, shuffle=True, seed=13)\n",
    "val_ds = create_dataset(f\"{SPLITS_DIR}/val.json\", img_size=IMG_SIZE, \n",
    "                        batch=BATCH_SIZE, shuffle=False)\n",
    "test_ds = create_dataset(f\"{SPLITS_DIR}/test.json\", img_size=IMG_SIZE, \n",
    "                         batch=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"✓ Data pipelines created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkvCdkW0FuZw"
   },
   "source": [
    "def build_model(input_shape=(224,224,3), num_classes=2, dropout=0.25):\n",
    "    \"\"\"\n",
    "    Transfer learning with MobileNetV2:\n",
    "    1. Data augmentation layer\n",
    "    2. Pre-trained MobileNetV2 (ImageNet weights, frozen initially)\n",
    "    3. Global Average Pooling\n",
    "    4. Dropout for regularization\n",
    "    5. Dense output layer with softmax\n",
    "    \"\"\"\n",
    "    # Data augmentation\n",
    "    aug = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.05),\n",
    "        layers.RandomZoom(0.10),\n",
    "        layers.RandomContrast(0.10),\n",
    "    ], name=\"augmentation\")\n",
    "    \n",
    "    # Pre-trained base model\n",
    "    base = keras.applications.MobileNetV2(\n",
    "        input_shape=input_shape, \n",
    "        include_top=False, \n",
    "        weights=\"imagenet\"\n",
    "    )\n",
    "    base.trainable = False  # Freeze during initial training\n",
    "    \n",
    "    # Build complete model\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = aug(inputs)\n",
    "    x = keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "    x = base(x, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name='mobilenet_concrete')\n",
    "    return model, base\n",
    "\n",
    "# Build model\n",
    "model, base_model = build_model(input_shape=IMG_SIZE + (3,), num_classes=2, dropout=0.25)\n",
    "\n",
    "# Display architecture\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters: {model.count_params():,}\")\n",
    "print(f\"Trainable parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Stage 1: Train with Frozen Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_stage1 = keras.callbacks.ModelCheckpoint(\n",
    "    f\"{OUTPUT_DIR}/stage1.keras\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=4,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train Stage 1\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 1: Training with frozen MobileNetV2 base\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history_stage1 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=6,\n",
    "    callbacks=[checkpoint_stage1, early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Stage 2: Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze top layers of base model\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-40]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"\\nFine-tuning last 40 layers of MobileNetV2\")\n",
    "print(f\"Trainable parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "checkpoint_stage2 = keras.callbacks.ModelCheckpoint(\n",
    "    f\"{OUTPUT_DIR}/stage2.keras\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train Stage 2\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2: Fine-tuning top layers\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history_stage2 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=6,\n",
    "    callbacks=[checkpoint_stage2, early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories from both stages\n",
    "combined_loss = history_stage1.history['loss'] + history_stage2.history['loss']\n",
    "combined_val_loss = history_stage1.history['val_loss'] + history_stage2.history['val_loss']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax.plot(combined_loss, label='Training Loss', linewidth=2)\n",
    "ax.plot(combined_val_loss, label='Validation Loss', linewidth=2)\n",
    "ax.axvline(x=len(history_stage1.history['loss']), color='red', \n",
    "           linestyle='--', label='Fine-tuning starts', alpha=0.7)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/loss_curves.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names, title, save_path):\n",
    "    \"\"\"Generate and save confusion matrix visualization\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel(\"Predicted Label\", fontsize=12)\n",
    "    ax.set_ylabel(\"True Label\", fontsize=12)\n",
    "    \n",
    "    # Set ticks\n",
    "    ax.set_xticks(range(len(class_names)))\n",
    "    ax.set_yticks(range(len(class_names)))\n",
    "    ax.set_xticklabels(class_names, fontsize=11)\n",
    "    ax.set_yticklabels(class_names, fontsize=11)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, f'{cm[i, j]:,}', \n",
    "                   ha='center', va='center',\n",
    "                   color='white' if cm[i, j] > thresh else 'black',\n",
    "                   fontsize=14, fontweight='bold')\n",
    "    \n",
    "    fig.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=160, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Get predictions\n",
    "y_test_true = np.concatenate([y.numpy() for _, y in test_ds], axis=0)\n",
    "y_test_prob = model.predict(test_ds, verbose=0)\n",
    "y_test_pred = y_test_prob.argmax(axis=1)\n",
    "\n",
    "class_names = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = plot_confusion_matrix(\n",
    "    y_test_true, \n",
    "    y_test_pred, \n",
    "    class_names,\n",
    "    \"Test Set Confusion Matrix\",\n",
    "    f\"{OUTPUT_DIR}/confusion_matrix.png\"\n",
    ")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives:  {cm[0,0]:,}\")\n",
    "print(f\"False Positives: {cm[0,1]:,}\")\n",
    "print(f\"False Negatives: {cm[1,0]:,}\")\n",
    "print(f\"True Positives:  {cm[1,1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test_true, y_test_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"{OUTPUT_DIR}/concrete_cnn_model.keras\")\n",
    "print(f\"\\n✓ Model saved to {OUTPUT_DIR}/concrete_cnn_model.keras\")\n",
    "\n",
    "# Save metrics to text file\n",
    "with open(f\"{OUTPUT_DIR}/metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Test Loss: {test_loss:.4f}\\n\")\n",
    "    f.write(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "    f.write(\"Test Confusion Matrix (rows=true, cols=pred):\\n\")\n",
    "    for row in cm:\n",
    "        f.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(f\"✓ Metrics saved to {OUTPUT_DIR}/metrics.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Conclusion & Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary\n",
    "- **Test Accuracy:** 99.92%\n",
    "- **Test Loss:** 0.0030\n",
    "- **Model:** MobileNetV2 with transfer learning + fine-tuning\n",
    "\n",
    "### Key Techniques Applied\n",
    "1. **Transfer Learning:** Leveraged ImageNet pre-trained MobileNetV2\n",
    "2. **Two-stage Training:** Initial frozen training followed by fine-tuning\n",
    "3. **Data Augmentation:** Random flips, rotations, zoom, and contrast adjustments\n",
    "4. **Regularization:** Dropout (0.25) to prevent overfitting\n",
    "5. **Stratified Splitting:** Maintained class balance across train/val/test sets\n",
    "\n",
    "### Model Performance\n",
    "The model achieves near-perfect classification with only 5 misclassifications \n",
    "out of 6000 test images:\n",
    "- 4 false positives (clean concrete classified as defective)\n",
    "- 1 false negative (defective concrete classified as clean)\n",
    "\n",
    "This high accuracy demonstrates the effectiveness of transfer learning for \n",
    "binary image classification tasks with sufficient training data.\n",
    "\n",
    "### Potential Improvements\n",
    "- Experiment with other architectures (EfficientNet, ResNet)\n",
    "- Implement ensemble methods combining multiple models\n",
    "- Add class activation maps (Grad-CAM) for interpretability\n",
    "- Deploy as REST API for real-time inference"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
